{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model Build On Top of VGG 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective\n",
    "Create a pipeline to feed a neural network image classifier utilizing the VGG 16 backbone.\n",
    "\n",
    "- Data:\n",
    "    - [Kaggle Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/overview)\n",
    "- Inputs:\n",
    "    - NumPy arrays\n",
    "- Outputs:\n",
    "    - predicted class name\n",
    "    - probability of predicted class as a percentage\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import WeightedRandomSampler, SubsetRandomSampler\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "FONT_SIZE = {\n",
    "    'label': 14,\n",
    "    'supertitle': 24,\n",
    "    'title': 20,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Images to NumPy Arrays\n",
    "\n",
    "This is done only as an exercise to setup a pipeline based on NumPy arrays in lieu of PIL images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(im_file: Path):\n",
    "    \"\"\"Convert image file to NumPy array.\"\"\"\n",
    "    arr_file = im_file.with_suffix('.npy')\n",
    "    if not arr_file.is_file():\n",
    "        im = PIL.Image.open(im_file)\n",
    "        np.save(arr_file, np.array(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'Cat_Dog_data'\n",
    "\n",
    "im_files = {x.resolve() for x in Path(data_dir).glob('**/*') \n",
    "            if x.suffix in ('.jpeg', '.jpg', '.png')}\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    executor.map(image_to_array, im_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean and Standard Deviation of Dataset\n",
    "\n",
    "Default ImageNet values over three channels:\n",
    "$$\\mu = (0.485, 0.456, 0.406)$$\n",
    "$$\\sigma = (0.229, 0.224, 0.225)$$\n",
    "\n",
    "These parameters are appropriate to use for transfer learning when the custom dataset is similar to the ImageNet dataset.\n",
    "\n",
    "#### Welford's Online Algorithm for Variance\n",
    "\n",
    "- *Online Algorithm*: An algorithm designed to process each new piece of data as it arrives to produce a final result without knowledge of any future data.\n",
    "- Calculates the standard deviation in one pass of the data eliminating the need to first cycle through the data to determine the mean.\n",
    "- Avoids the error found in naive variance calculations when the standard deviation is much smaller than the mean.\n",
    "\n",
    "[Welford, B.P., 1962. Note on a method for calculating corrected sums of squares and products. Technometrics, 4(3), pp.419-420.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.302.7503&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Datasets\n",
    "\n",
    "**NOTE**: The images are not resized since they will be yielded from the loader with a batch size of one for the mean and standard caclculation.\n",
    "\n",
    "##### Datasets From Images Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_im_data = datasets.ImageFolder(f'{data_dir}/train',\n",
    "                                     transform=transforms.ToTensor())\n",
    "test_im_data = datasets.ImageFolder(f'{data_dir}/test',\n",
    "                                    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datasets From NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npy_loader(path: Path):\n",
    "    \"\"\"Helper function to load NumPy files into DataLoader.\"\"\"\n",
    "    arr = np.load(path).astype(np.float64)\n",
    "    np.divide(arr, 255.0, out=arr)\n",
    "    arr = np.moveaxis(arr, 2, 0)\n",
    "    return torch.from_numpy(arr).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.DatasetFolder(\n",
    "    root=f'{data_dir}/train',\n",
    "    loader=npy_loader,\n",
    "    extensions=('.npy'),\n",
    ")\n",
    "test_data = datasets.DatasetFolder(\n",
    "    root=f'{data_dir}/test', \n",
    "    loader=npy_loader,\n",
    "    extensions=('.npy'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Welford's Online Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_mean_std(loader):\n",
    "    \"\"\"Compute the mean and standard deviation in an online fashion.\"\"\"\n",
    "    px_cnt = 0\n",
    "    moment_0 = torch.empty(3)\n",
    "    moment_1 = torch.empty(3)\n",
    "\n",
    "    for data in loader:\n",
    "        data = data[0]\n",
    "        batch, chanels, height, width = data.shape\n",
    "        total_pixels = px_cnt + batch * height * width\n",
    "        channel_sum = torch.sum(data, dim=[0, 2, 3])\n",
    "        channel_sum_squares = torch.sum(data ** 2, dim=[0, 2, 3])\n",
    "        moment_0 = (px_cnt * moment_0 + channel_sum) / total_pixels\n",
    "        moment_1 = (px_cnt * moment_1 + channel_sum_squares) / total_pixels\n",
    "        px_cnt = total_pixels\n",
    "\n",
    "    return moment_0, torch.sqrt(moment_1 - moment_0 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = namedtuple('Stats', 'mean, std')\n",
    "\n",
    "stats = {}\n",
    "for dataset in (train_data, test_data):\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        num_workers=1,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    name = dataset.root.split('/')[-1]\n",
    "    stats[name] = Stats(*online_mean_std(loader))\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    Channel Statistics\n",
    "    \n",
    "    Mean:\n",
    "    \\tTrain: {[f'{x:6f}' for x in stats['train'].mean.tolist()]}\n",
    "    \\tTest:  {[f'{x:6f}' for x in stats['test'].mean.tolist()]}\n",
    "    \n",
    "    Standard Deviation\n",
    "    \\tTrain: {[f'{x:6f}' for x in stats['train'].std.tolist()]}\n",
    "    \\tTest:  {[f'{x:6f}' for x in stats['test'].std.tolist()]}\n",
    "    \"\"\"\n",
    "    .replace(\"'\", '')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_transforms = [\n",
    "    transforms.ColorJitter(\n",
    "        brightness=(0.1, 0.9),\n",
    "        contrast=0.5,\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "]\n",
    "shared_transforms = [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=stats['train'].mean, std=stats['train'].std),\n",
    "]\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    ([transforms.ToPILImage(),\n",
    "      transforms.RandomResizedCrop(224),\n",
    "      transforms.RandomApply(random_transforms, p=0.5),\n",
    "     ]\n",
    "     + shared_transforms)\n",
    ") \n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    ([transforms.ToPILImage(),\n",
    "      transforms.Resize(255),\n",
    "      transforms.CenterCrop(224),\n",
    "     ]\n",
    "     + shared_transforms)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Datasets\n",
    "\n",
    "##### Datasets From Images Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_im_data = datasets.ImageFolder(f'{data_dir}/train',\n",
    "                                     transform=train_transforms)\n",
    "test_im_data = datasets.ImageFolder(f'{data_dir}/test',\n",
    "                                    transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datasets From NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.DatasetFolder(\n",
    "    root=f'{data_dir}/train',\n",
    "    loader=npy_loader,\n",
    "    extensions=('.npy'),\n",
    "    transform=train_transforms,\n",
    ")\n",
    "test_data = datasets.DatasetFolder(\n",
    "    root=f'{data_dir}/test', \n",
    "    loader=npy_loader,\n",
    "    extensions=('.npy'),\n",
    "    transform=test_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets_df(targets: Iterable[int],\n",
    "               data: 'DatasetFolder') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert targets for dataset to data frame.\n",
    "    \n",
    "    :param targets: classification targets\n",
    "    :param data: parent dataset\n",
    "    \"\"\"\n",
    "    df = (pd.DataFrame(targets, columns=['target'])\n",
    "          .rename_axis('example')\n",
    "         )\n",
    "    df['class'] = (df['target']\n",
    "                   .map({v: k for k, v in data.class_to_idx.items()})\n",
    "                   .astype('category')\n",
    "                  )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Class Weights for Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "workers = 2\n",
    "\n",
    "train_df = targets_df(train_data.targets, train_data)\n",
    "class_sample_cnt = train_df.groupby('target')['target'].size()\n",
    "weights = 1 / class_sample_cnt\n",
    "train_df['weights'] = train_df['target'].map(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Train Dataset into Stratified Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "\n",
    "train_mask = (train_df\n",
    "              .sample(frac=train_frac, weights='weights')\n",
    "              .index\n",
    "             )\n",
    "train_df.loc[train_mask, 'split'] = 'train'\n",
    "train_df['split'] = train_df['split'].fillna('valid').astype('category')\n",
    "\n",
    "train_idx, valid_idx = [train_df.query(f\"split == '{x}'\").index.tolist()\n",
    "                        for x in ('train', 'valid')]\n",
    "\n",
    "train_sampler = WeightedRandomSampler(\n",
    "    weights=torch.tensor(train_df['weights']),\n",
    "    num_samples=batch_size,\n",
    "    replacement=False,\n",
    ")\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=batch_size,\n",
    "                         num_workers=workers, sampler=train_sampler)\n",
    "validloader = DataLoader(train_data, batch_size=batch_size,\n",
    "                         num_workers=workers, sampler=valid_sampler)\n",
    "testloader = DataLoader(test_data, batch_size=batch_size,\n",
    "                        num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Train Validation Class Split with Sample Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure('Class Balance', figsize=(10, 8),\n",
    "           facecolor='white', edgecolor='black')\n",
    "rows, cols = (2, 2)\n",
    "ax0 = plt.subplot2grid((rows, cols), (0, 0))\n",
    "ax1 = plt.subplot2grid((rows, cols), (0, 1))\n",
    "ax2 = plt.subplot2grid((rows, cols), (1, 0))\n",
    "ax3 = plt.subplot2grid((rows, cols), (1, 1))\n",
    "\n",
    "sns.countplot(\n",
    "    x='class',\n",
    "    hue='split',\n",
    "    data=train_df,\n",
    "    palette=sns.color_palette('Paired', n_colors=2),\n",
    "    ax=ax0\n",
    ")\n",
    "ax0.set_title('Train / Validation Class Count', fontsize=FONT_SIZE['title'])\n",
    "\n",
    "for inputs, labels in trainloader:\n",
    "    batch_df = targets_df(labels.numpy(), train_data)\n",
    "    sns.countplot(\n",
    "        x='class',\n",
    "        data=batch_df,\n",
    "        alpha=0.7,\n",
    "        palette=sns.color_palette('Reds',\n",
    "                                  n_colors=batch_df['class'].unique().size),\n",
    "        ax=ax1,\n",
    "    )\n",
    "ax1.set_title('Sample Train Batch Class Count', fontsize=FONT_SIZE['title'])\n",
    "    \n",
    "sns.countplot(\n",
    "    x='class',\n",
    "    data=train_df.query(\"split == 'train'\"),\n",
    "    color=sns.color_palette('Paired', n_colors=2).as_hex()[0],\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title('Train Class Distribution', fontsize=FONT_SIZE['title'])\n",
    "\n",
    "sns.countplot(\n",
    "    x='class',\n",
    "    data=train_df.query(\"split == 'valid'\"),\n",
    "    color=sns.color_palette('Paired', n_colors=2).as_hex()[1],\n",
    "    ax=ax3\n",
    ")\n",
    "ax3.set_title('Validation Class Distribution', fontsize=FONT_SIZE['title'])\n",
    "\n",
    "for ax in (ax0, ax1, ax2, ax3):\n",
    "    ax.set_xlabel('')\n",
    "    ax.tick_params(axis='x', which='major', labelsize=FONT_SIZE['label'])\n",
    "    ax.set_ylabel('Count', fontsize=FONT_SIZE['label'])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    " \n",
    "for p in ax0.patches:\n",
    "    height = p.get_height()\n",
    "    ax0.text(p.get_x() + p.get_width() / 2,\n",
    "             height + 100,\n",
    "             f'{height:1.0f}',\n",
    "             ha=\"center\")\n",
    "\n",
    "for p in ax1.patches:\n",
    "    height = p.get_height()\n",
    "    ax1.text(p.get_x() + p.get_width() / 2,\n",
    "             height + 0.3,\n",
    "             f'{height:1.0f} ({height / batch_size * 100:1.0f}%)',\n",
    "             ha=\"center\")\n",
    "\n",
    "for ax, length in ((ax2, len(train_idx)), (ax3, len(valid_idx))):    \n",
    "    ax.set_ylabel('', fontsize=FONT_SIZE['label'])\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        ax.text(p.get_x() + p.get_width() / 2,\n",
    "                height + height / 100,\n",
    "                f'{height / length * 100:1.0f}%',\n",
    "                ha=\"center\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Plot image from Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy()\n",
    "    image = np.moveaxis(image, 0, 2)\n",
    "    \n",
    "    if normalize:\n",
    "        mean = stats['train'].mean.numpy()\n",
    "        std = stats['train'].std.numpy()\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    for border in ('top', 'right', 'left', 'bottom'):\n",
    "        ax.spines[border].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples From Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {\n",
    "    'train': {'data': train_data, 'loader': trainloader},\n",
    "    'valid': {'data': train_data, 'loader': validloader},\n",
    "    'test': {'data': test_data, 'loader': testloader},\n",
    "}\n",
    "\n",
    "cols = 5\n",
    "for row, dataset in enumerate(sets):\n",
    "    dataiter = iter(sets[dataset]['loader'])\n",
    "    images, labels = dataiter.next()\n",
    "    fig, ax = plt.subplots(figsize=(15, 4), ncols=cols)\n",
    "    for idx in range(cols):\n",
    "        category = sets[dataset]['data'].classes[labels[idx]]\n",
    "        plot_image(\n",
    "            images[idx],\n",
    "            ax=ax[idx],\n",
    "            title=category,\n",
    "        )\n",
    "        plt.suptitle(f'{dataset.capitalize()} Examples:',\n",
    "                     fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Pixel Values for Test Image\n",
    "\n",
    "#### WARNING\n",
    "Execution of this cell is on the order of minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(sets[dataset]['loader'])\n",
    "images, labels = dataiter.next()\n",
    "rgb_im = np.squeeze(images[0].numpy())\n",
    "titles = [f'{x} Channel' for x in ('Red', 'Green', 'Blue')]\n",
    "\n",
    "fig = plt.figure(figsize=(1200, 1200))\n",
    "for idx in range(3):\n",
    "    ax = fig.add_subplot(1, 3, idx + 1)\n",
    "    im = rgb_im[idx]\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.set_title(titles[idx])\n",
    "    width, height = im.shape\n",
    "    thresh = im.max() / 2.5\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            val = round(im[x][y], 2) if im[x][y] != 0 else 0\n",
    "            ax.annotate(str(val), xy=(y, x),\n",
    "                        horizontalalignment='center',\n",
    "                        verticalalignment='center',\n",
    "                        size=8,\n",
    "                        color='white' if im[x][y] < thresh else 'black')\n",
    "plt.savefig('test_image_detail.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Network Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 16 Backbone\n",
    "\n",
    "Load the model and freeze the parameters so backprop will not apply to the backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure for CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Hardware Execution Mode: {str(device).upper()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Architecture\n",
    "\n",
    "Log softmax will be the output of the network to allow easy access to class probabilities during the evaulation step.\n",
    "This results in the criterion being the negative log likelihood loss `NLLLoss`.\n",
    "\n",
    "If the `CrossEntropyLoss` were to be used then the outputs would be values from the logits and which would require a transformation to yield probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"New classifier layers for model.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.category_cnt = 15\n",
    "        input_size = 25088\n",
    "        hidden_1 = 4096\n",
    "        self.fc1 = nn.Linear(input_size, hidden_1)\n",
    "        self.output = nn.Linear(hidden_1, self.category_cnt)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Define forward pass through layers.\"\"\"\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = F.log_softmax(self.output(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "model.classifier = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(\n",
    "    model.classifier.parameters(),\n",
    "    lr=0.003,\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train Network\n",
    "\n",
    "#### Note:\n",
    "The variable `eval_freq` determines how frequently the *entire* validation set will be evaluated.\n",
    "Reducing this parameter will result in more evaluations and in turn increase training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "eval_freq = len(train_data) // 2\n",
    "output_dir = Path()\n",
    "checkpoint_file = output_dir / 'custom_VGG16.pth'\n",
    "\n",
    "running_loss = 0\n",
    "steps = 0\n",
    "valid_loss_min = np.Inf\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = [x.to(device) for x in (inputs, labels)]        \n",
    "\n",
    "        logps = model(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "     \n",
    "        steps += 1\n",
    "        if steps % eval_freq == 0:\n",
    "            \n",
    "            model.eval()\n",
    "            valid_loss = 0\n",
    "            accuracy = 0            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in validloader:\n",
    "                    inputs, labels = [x.to(device) for x in (inputs, labels)]\n",
    "        \n",
    "                    logps = model(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    valid_loss += batch_loss.item()\n",
    "                    \n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += (torch.mean(equals.type(torch.FloatTensor))\n",
    "                                 .item())\n",
    "            \n",
    "            train_loss = running_loss / (batch_size * eval_freq)\n",
    "            valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "            accuracy = accuracy / len(valid_loader.sampler)\n",
    "            print(\n",
    "                f\"\"\"\n",
    "                Epoch:               {epoch + 1}/{epochs}\n",
    "                Train Loss:          {train_loss:.3f}\n",
    "                Validation Loss:     {valid_loss:.3f}\n",
    "                Validation Accuracy: {accuracy * 100:.2f}%\n",
    "                \n",
    "                \"\"\")\n",
    "            \n",
    "            if valid_loss <= valid_loss_min:\n",
    "                print(f'Validation loss decreased: '\n",
    "                      f'{valid_loss_min:.6f} --> {valid_loss:.6f}\\n'\n",
    "                      f'\\tSaving model ...')\n",
    "\n",
    "                torch.save(model.state_dict(), checkpoint_file)\n",
    "                valid_loss_min = valid_loss\n",
    "            \n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_file))\n",
    "\n",
    "test_loss = 0.0\n",
    "label_map = {n: c for n, c in enumerate(test_data.classes)}\n",
    "correct = Counter()\n",
    "total = Counter()\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = [x.to(device) for x in (inputs, labels)]\n",
    "\n",
    "        logps = model(inputs)\n",
    "        batch_loss = criterion(logps, labels)\n",
    "        test_loss += batch_loss.item()\n",
    "\n",
    "        ps = torch.exp(logps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        \n",
    "        for label in labels:\n",
    "            category = str(label_map[label.item()])\n",
    "            correct.update(label)\n",
    "            total.update(label)\n",
    "        \n",
    "test_loss = test_loss / len(test_loader.sampler)\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "    Test Dataset\n",
    "    Loss:           {test_loss:.6f}\n",
    "    Total Accuracy: {sum(correct.values / sum(total.values) * 100)}%\n",
    "    \"\"\")\n",
    "\n",
    "print('Accuracy by Class')\n",
    "for c in range(test_data.classes):\n",
    "    print(f'\\t{c}: {correct[c] / total[c] * 100:1.0f}% '\n",
    "          f'({correct[c]}/{total[c]})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
