{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timothy Helton\n",
    "\n",
    "2018-06-26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Background\n",
    "\n",
    "- This algorithm is ideal for **binary** classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nomenclature\n",
    "\n",
    "- b: intercept parameter\n",
    "- J: cost function\n",
    "- L: loss function\n",
    "- w: weight parameters\n",
    "- X: matrix of all input vector instances\n",
    "    - each column is an instance\n",
    "- x: input vector for single instance\n",
    "- $\\hat{y}$ or a: predicted output\n",
    "- $\\sigma$: sigmoid function or logistic function\n",
    "    - $\\sigma(z) = \\frac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "1. A forward pass through the model is used to calculate the cost function.\n",
    "1. A backwards pass through the model evaluating the gradient facilitates the training of parameters (w and b).\n",
    "1. Once the parameters are trained the model is evaluated against the test dataset.\n",
    "1. If satisfied with the results from the training set apply the model to the validation set.\n",
    "1. Finally, the model is applied to unclassified data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graph\n",
    "\n",
    "![Logistic Regression Computational Graph](images/logistic_regression_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "- b: intercept parameter\n",
    "    - $\\in\\mathbb{R}$\n",
    "- w: weight parameters\n",
    "    - $\\in\\mathbb{R}^{n_x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid or Logistic Function\n",
    "The ***sigmoid*** or ***logistic*** function used to be the standard.\n",
    "    - On the edges of the function the gradient near zero results in slow learning.\n",
    "    - Now the ReLU function outperforms the sigmoid.\n",
    "    \n",
    "**Sigmoid Function**\n",
    "![Sigmoid Function](images/sigmoid_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified Linear Unit (ReLU)\n",
    "![ReLU Function](images/ReLU_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent Function\n",
    "Similar to the sigmoid function.\n",
    "- Centered about 0 instead of 0.5\n",
    "- Ranges from -1 to 1\n",
    "- The derivative is: $1 - a^2$ which is adds computational efficiency\n",
    "\n",
    "$tanh = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "\n",
    "![tanh Function](images/tanh_function.gif)\n",
    "\n",
    "$tanh \\frac{d}{dz} = (e^z - e^{-z})(e^z + e^{-z})^{-1} \\frac{d}{dz}$\n",
    "\n",
    "**Product Rule**: $f(z)g(z) = f'(z)g(z) + g'(z)f(z)$\n",
    "\n",
    "$f(z) = e^z - e^{-z}$\n",
    "\n",
    "$f'(z) = e^z + e^{-z}$\n",
    "\n",
    "$g(z) = (e^z + e^{-z})^{-1}$\n",
    "\n",
    "**Chain Rule**: $m(n(z)) = m \\frac{d}{dn} n \\frac{d}{dz}$\n",
    "\n",
    "$m = n^{-1}$\n",
    "\n",
    "$n = e^z + e^{-z}$\n",
    "\n",
    "$m' = -n^{-2} = -(e^z + e^{-z})^{-2}$\n",
    "\n",
    "$n' = e^z - e^{-z}$\n",
    "\n",
    "$g'(z) = -(e^z + e^{-z})^{-2}(e^z - e^{-z})$\n",
    "\n",
    "**Substitute Results**\n",
    "\n",
    "$tanh \\frac{d}{dz} = (e^z + e^{-z}) (e^z + e^{-z})^{-1} - (e^z + e^{-z})^{-2} (e^z - e^{-z})(e^z - e^{-z})$\n",
    "\n",
    "$tanh \\frac{d}{dz} = \\frac{e^z + e^{-z}}{e^z + e^{-z}} - \\frac{(e^z - e^{-z})^2}{(e^z + e^{-z})^2}$\n",
    "\n",
    "$tanh \\frac{d}{dz} = 1 - \\frac{(e^z - e^{-z})^2}{(e^z + e^{-z})^2}$\n",
    "\n",
    "$tanh \\frac{d}{dz} = 1 - tanh^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "- The average of all the loss functions.\n",
    "- Used to train the parameters w and b.\n",
    "    - Influence of b on L\n",
    "        - $\\frac{\\partial L}{\\partial b}$\n",
    "    - Influence of w on L\n",
    "        - $\\frac{\\partial L}{\\partial w_i}$\n",
    "        \n",
    "$J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(a, y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "$L(a, y) = -(y \\log{a} + (1 - y) \\log{(1 - a)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of Loss with respect to Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(a, y) = -(y \\log{a} + (1 - y) \\log{(1 - a)})$\n",
    "\n",
    "$\\frac{dL}{da} = -y \\log(a) \\frac{d}{da} - (1 - y) \\log(1 - a) \\frac{d}{da}$\n",
    "\n",
    "1. $y \\log(a) \\frac{d}{da} = \\frac{y}{a}$\n",
    "\n",
    "1. $(1 - y) \\log(1 - a) \\frac{d}{da}$\n",
    "    - Requires the Chain Rule\n",
    "        - $f(g(a)) = (1 - y)\\log(1 - a)$\n",
    "        - $f(g) = (1 - y)\\log(g)$\n",
    "        - $g(a) = 1 - a$\n",
    "        - $f \\frac{d}{dg} = \\frac{1 - y}{g}$\n",
    "        - $g \\frac{d}{da} = -1$\n",
    "        - $f \\frac{d}{dg} * g \\frac{d}{da} = - \\frac{1 - y}{g} = - \\frac{1 - y}{1 - a}$\n",
    "\n",
    "$\\frac{dL}{da} = -\\frac{y}{a} - -\\frac{1 - y}{1 - a}$\n",
    "\n",
    "$\\frac{dL}{da} = \\frac{1 - y}{1 - a} - \\frac{y}{a}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of Activation with respect to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{da}{dz} = \\sigma \\frac{d}{dz}$\n",
    "\n",
    "$\\frac{da}{dz} = (1 + e^{-z})^{-1} \\frac{d}{dz}$\n",
    "\n",
    "1. Requires Chain Rule\n",
    "    - $f(g) = g^{-1}$\n",
    "    - $g(z) = 1 + e^{-z}$\n",
    "    - $f(g) \\frac{d}{dg} = -g^{-2}$\n",
    "    - $g(z) \\frac{d}{dz} = -e^{-z}$\n",
    "    - $f \\frac{d}{dg} * g \\frac{d}{dz} = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "Trick: Add and subtract 1 to the numerator.\n",
    "\n",
    "$\\frac{e^{-z} + 1 - 1}{(1 + e^{-z})^2}$\n",
    "\n",
    "$\\frac{1 + e^{-z}}{(1 + e^{-z})^2} - \\frac{1}{(1 + e^{-z})^2}$\n",
    "\n",
    "$\\frac{1}{(1 + e^{-z})} - \\frac{1}{(1 + e^{-z})^2}$\n",
    "\n",
    "$\\frac{1}{(1 + e^{-z})} \\Big( 1 - \\frac{1}{(1 + e^{-z})} \\Big)$\n",
    "\n",
    "$a(1 - a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of Loss with respect to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{dL}{dz} = \\frac{dL}{da} \\frac{da}{dz}$\n",
    "\n",
    "$\\frac{dL}{dz} = \\Big( \\frac{1 - y}{1 - a} - \\frac{y}{a} \\Big) a(1 - a)$\n",
    "\n",
    "$\\frac{dL}{dz} = \\frac{a(1 - a)(1 - y)}{1 - a} - \\frac{a(1 - a)y}{a}$\n",
    "\n",
    "$\\frac{dL}{dz} = a(1 - y) - y(1 - a)$\n",
    "\n",
    "$\\frac{dL}{dz} = a - ay - y + ay$\n",
    "\n",
    "$\\frac{dL}{dz} = a - y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of Model with respect to Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = wx + b$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial w} = x$\n",
    "\n",
    "$\\frac{\\partial z}{\\partial b} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of Loss with respect to Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**w**\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = \\frac{dL}{da} \\frac{da}{dz} \\frac{\\partial z}{\\partial w}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = \\frac{dL}{dz} \\frac{\\partial z}{\\partial w}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = (a - y) x$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = x(a - y)$\n",
    "\n",
    "\n",
    "**b**\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{dL}{da} \\frac{da}{dz} \\frac{\\partial z}{\\partial b}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{dL}{dz} \\frac{\\partial z}{\\partial b}$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = (a - y) 1$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = (a - y)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "- $\\hat{y}$ or a: predicted output\n",
    "    - probability that y is equal to 1 given x\n",
    "        - $\\hat{y} = P(y=1 | x)$\n",
    "    - apply the sigmoid function to return a value between 0 and 1\n",
    "        - $\\hat{y} = \\sigma(w^{T}x + b)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
